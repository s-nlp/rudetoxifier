{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detoxGPT_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asIWi_MThhxP"
      },
      "source": [
        "### Install required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s472U7xhHQB"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install tensorflow_text\n",
        "!pip3 install urllib3==1.25.4\n",
        "!pip3 install transformers==2.8.0\n",
        "!pip install --upgrade pip\n",
        "!wget -N https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuaPOAeDhQbE"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# !wget https://www.dropbox.com/s/ei5vw6cbd9fragp/dataset_200.xlsx\n",
        "\n",
        "# data = pd.read_excel('dataset_200.xlsx')\n",
        "# data.dropna(inplace=True)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# delimiter = '>>>'\n",
        "\n",
        "# sentences = data.apply(lambda x: x[0] + f' {delimiter} ' + x[1], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-1aPIvhoQX"
      },
      "source": [
        "### Arguments for generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUVujZNoht9F"
      },
      "source": [
        "from generate_transformers import *\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
        "\n",
        "        self.prompt = ''\n",
        "        self.length = 50\n",
        "        self.stop_token = '</s>'\n",
        "\n",
        "        self.k = 5\n",
        "        self.p = .95\n",
        "        self.temperature = 1\n",
        "\n",
        "        self.repetition_penalty = 1\n",
        "        self.num_return_sequences = 1\n",
        "\n",
        "        self.device='cuda'\n",
        "        self.seed=42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRZvaeluhqt0"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0jTgGHbhIAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e62120d-6445-4c1e-84fd-091662324bf8"
      },
      "source": [
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def generate_sequences(prompt_text, args, delimiter='>>>'):\n",
        "    args.prompt_text = prompt_text\n",
        "\n",
        "    \n",
        "    if prompt_text.endswith('.txt'):\n",
        "      with open(prompt_text, 'r') as f:\n",
        "        prompt_text = f.read()\n",
        "\n",
        "    # print(f'Input:\\n{prompt_text}\\n')\n",
        "    \n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=encoded_prompt,\n",
        "        max_length=args.length + len(encoded_prompt[0]),\n",
        "        temperature=args.temperature,\n",
        "        top_k=args.k,\n",
        "        top_p=args.p,\n",
        "        repetition_penalty=args.repetition_penalty,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=args.num_return_sequences,\n",
        "    )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "            output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "        text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "        if delimiter in text:\n",
        "            text = text.split(delimiter)[0].rstrip()\n",
        "        else:\n",
        "            text = text.split('\\n')[0].rstrip()\n",
        "\n",
        "        generated_sequences.append(text)\n",
        "        # print(f'[{generated_sequence_idx}]ruGPT:\\n{prompt_text.split('\\n')[-1] + text}')\n",
        "\n",
        "    return generated_sequences\n",
        "\n",
        "\n",
        "def compute_use(target_comment, generated_comments):\n",
        "    target_comment = embed(list([target_comment]))\n",
        "    generated_comments = list(map(embed, generated_comments))\n",
        "\n",
        "    return [np.inner(target_comment, gc)[0][0] for gc in generated_comments]\n",
        "\n",
        "\n",
        "def compare_results(source_comment, target_comment, generated_comments, scores):\n",
        "    print(f'Toxic : {source_comment}')\n",
        "    print(f'Polite: {target_comment}\\n')\n",
        "\n",
        "    print(f'Score  Generated Comment')\n",
        "    for i in np.argsort(scores):\n",
        "        print(np.round(scores[i], 3), generated_comments[i])\n",
        "    return generated_comments[np.argsort(scores)[0]], np.argsort(scores)[0]\n",
        "\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/16/2021 06:36:35 - INFO - absl -   Using /tmp/tfhub_modules to cache modules.\n",
            "06/16/2021 06:36:35 - INFO - absl -   Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.\n",
            "06/16/2021 06:36:38 - INFO - absl -   Downloaded https://tfhub.dev/google/universal-sentence-encoder-multilingual/3, Total size: 266.88MB\n",
            "06/16/2021 06:36:38 - INFO - absl -   Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI4cmKLjhtjb"
      },
      "source": [
        "### Download weights & unzip "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCTrevrFjTcB",
        "outputId": "f2159e28-f026-47be-8b35-0265d4d08e2b"
      },
      "source": [
        "!gdown --id 1RYUku5_MWXZF2xlIpOTZmi9_DH-SG0lz && mkdir rugpt3_large_200 && unzip rugpt3_large_200.zip -d rugpt3_large_200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RYUku5_MWXZF2xlIpOTZmi9_DH-SG0lz\n",
            "To: /content/rugpt3_large_200.zip\n",
            "1.83GB [00:15, 118MB/s] \n",
            "Archive:  rugpt3_large_200.zip\n",
            " extracting: rugpt3_large_200/training_args.bin  \n",
            " extracting: rugpt3_large_200/config.json  \n",
            " extracting: rugpt3_large_200/tokenizer_config.json  \n",
            " extracting: rugpt3_large_200/merges.txt  \n",
            " extracting: rugpt3_large_200/pytorch_model.bin  \n",
            " extracting: rugpt3_large_200/special_tokens_map.json  \n",
            " extracting: rugpt3_large_200/vocab.json  \n",
            "   creating: rugpt3_large_200/.ipynb_checkpoints/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmpTmHcKhxQE"
      },
      "source": [
        "### Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aKLOaZliQaF"
      },
      "source": [
        "args = Args()\n",
        "args.model_name_or_path = 'rugpt3_large_200'\n",
        "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
        "model = model_class.from_pretrained(args.model_name_or_path)\n",
        "model.to(args.device)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP2zK8p7h5Ut"
      },
      "source": [
        "text = 'Ты дурак и ничего не понимаешь. Что значит по-твоему построить дорогу?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z44ibmDriWpt",
        "outputId": "282031aa-d431-4202-92aa-dbadf6f94cf8"
      },
      "source": [
        "from tqdm import tqdm \n",
        "import re\n",
        "\n",
        "results = []\n",
        "\n",
        "# parameters\n",
        "args.num_return_sequences = 10\n",
        "args.k = 3\n",
        "args.p = .5\n",
        "args.temperature = 10\n",
        "# here text stands for your sentence\n",
        "args.length = len(text) + 5\n",
        "\n",
        "\n",
        "generated_sequences = generate_sequences(text + ' >>> ', args)\n",
        "results.append([re.sub('<pad>', '', x) for x in generated_sequences])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/16/2021 06:41:37 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw6iYBgVjAEu",
        "outputId": "79fd4367-7860-4083-e8ac-890362f13cb1"
      },
      "source": [
        "print(results[0][2][:args.length])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Вы глупец и ничего не понимаете. Что значит по-вашему построить дорогу?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0MLZUgwjc1y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}